"""
YOLO Duct Segmentation - Sistema de Identifica√ß√£o de Sess√µes de Dutos
Vers√£o corrigida e otimizada para detec√ß√£o de sess√µes em dutos quadrados de 50cm
"""

import cv2
import numpy as np
from ultralytics import YOLO
import argparse
import time
import os
from pathlib import Path
from collections import defaultdict, deque
import json
import math

class DuctDetectorLocal:
    def __init__(self, model_path, confidence_threshold=0.5):
        """
        Inicializa o detector de sess√µes de dutos
        
        Args:
            model_path: Caminho para o modelo YOLO treinado
            confidence_threshold: Limiar de confian√ßa para detec√ß√µes
        """
        self.model_path = model_path
        self.confidence_threshold = confidence_threshold
        
        # Carrega o modelo YOLO
        print(f"üîÑ Carregando modelo de: {model_path}")
        try:
            self.model = YOLO(model_path)
            print("‚úÖ Modelo carregado com sucesso!")
        except Exception as e:
            print(f"‚ùå Erro ao carregar modelo: {e}")
            raise
        
        # Configura√ß√µes da c√¢mera (valores padr√£o - ajustar conforme necess√°rio)
        self.camera_matrix = None
        self.frame_width = None
        self.frame_height = None
        
        # Hist√≥rico de rastreamento
        self.track_history = defaultdict(lambda: deque(maxlen=30))
        self.session_positions = {}  # Posi√ß√µes 3D das sess√µes
        self.camera_poses = [np.eye(4)]  # Hist√≥rico de poses da c√¢mera
        
        # M√©tricas de performance
        self.fps_counter = 0
        self.fps_start_time = time.time()
        self.avg_fps = 0
        
        # CONSTANTES DO DUTO
        self.DUCT_SIZE = 0.50  # Duto quadrado de 50cm
        self.DUCT_HALF_SIZE = self.DUCT_SIZE / 2  # 25cm do centro
        self.DUCT_TOLERANCE = 0.02  # 2cm de toler√¢ncia
        
        # Constantes de movimento do drone
        self.MAX_DRONE_SPEED = 1.0  # m/s velocidade m√°xima
        self.MAX_FRAME_MOVEMENT = 0.05  # 5cm movimento m√°ximo por frame
        
        # Configura√ß√µes de sess√µes
        self.MIN_SESSION_SPACING = 0.8  # Espa√ßamento m√≠nimo entre sess√µes (80cm)
        self.MAX_SESSION_SPACING = 1.5  # Espa√ßamento m√°ximo entre sess√µes (150cm)
        self.SESSION_WIDTH_RATIO = 0.85  # Sess√µes ocupam ~85% da largura do duto
        self.MAX_SESSIONS_VISIBLE = 8  # M√°ximo de sess√µes vis√≠veis simultaneamente
        
        # Dire√ß√£o atual do duto
        self.current_direction = np.array([0, 0, 1])  # Inicialmente para frente (Z)
        self.direction_history = deque(maxlen=10)
        
        print(f"üîß Configura√ß√µes do duto:")
        print(f"   Tamanho: {self.DUCT_SIZE}x{self.DUCT_SIZE}m")
        print(f"   Espa√ßamento de sess√µes: {self.MIN_SESSION_SPACING}-{self.MAX_SESSION_SPACING}m")
        print(f"   M√°ximo de sess√µes vis√≠veis: {self.MAX_SESSIONS_VISIBLE}")
    
    def set_camera_calibration(self, frame_width, frame_height):
        """Define calibra√ß√£o da c√¢mera baseada no tamanho do frame"""
        # Valores aproximados - idealmente calibrar com padr√£o de xadrez
        fx = fy = frame_width * 0.8  # Aproxima√ß√£o focal length
        cx = frame_width / 2
        cy = frame_height / 2
        
        self.camera_matrix = np.array([
            [fx, 0, cx],
            [0, fy, cy],
            [0, 0, 1]
        ], dtype=np.float32)
        
        print(f"üì∑ Calibra√ß√£o da c√¢mera definida:")
        print(f"   Resolu√ß√£o: {frame_width}x{frame_height}")
        print(f"   Focal length: {fx:.1f}")
    
    def estimate_depth_from_bbox(self, bbox_area, frame_width, frame_height):
        """
        Estima profundidade baseada no tamanho da bounding box
        
        Para sess√µes em duto de 50cm:
        - Muito pr√≥ximo (30cm): sess√£o ocupa ~70% da largura
        - Pr√≥ximo (60cm): sess√£o ocupa ~50% da largura
        - M√©dio (100cm): sess√£o ocupa ~30% da largura
        - Longe (150cm): sess√£o ocupa ~20% da largura
        """
        frame_area = frame_width * frame_height
        bbox_ratio = bbox_area / frame_area
        
        # Largura esperada da sess√£o (85% do duto = 42.5cm)
        session_width_m = self.DUCT_SIZE * self.SESSION_WIDTH_RATIO
        
        # Calcula profundidade baseada na propor√ß√£o
        if bbox_ratio > 0.3:  # Muito pr√≥ximo
            depth = 0.3
        elif bbox_ratio > 0.2:  # Pr√≥ximo
            depth = 0.6
        elif bbox_ratio > 0.1:  # M√©dio
            depth = 1.0
        elif bbox_ratio > 0.05:  # Longe
            depth = 1.5
        else:  # Muito longe
            depth = 2.0
        
        # Limites f√≠sicos do duto
        depth = max(depth, 0.2)  # M√≠nimo 20cm
        depth = min(depth, 3.0)  # M√°ximo 3m (limite de visibilidade)
        
        return depth
    
    def pixel_to_world_constrained(self, center_2d, depth, camera_pose, frame_width, frame_height):
        """
        Converte coordenadas de pixel para mundo com restri√ß√µes do duto
        """
        if self.camera_matrix is None:
            self.set_camera_calibration(frame_width, frame_height)
        
        # Par√¢metros da c√¢mera
        fx, fy = self.camera_matrix[0,0], self.camera_matrix[1,1]
        cx, cy = self.camera_matrix[0,2], self.camera_matrix[1,2]
        
        # Converte para coordenadas da c√¢mera
        x_cam = (center_2d[0] - cx) * depth / fx
        y_cam = (center_2d[1] - cy) * depth / fy
        z_cam = depth
        
        # RESTRI√á√ÉO CR√çTICA: Limita √†s dimens√µes do duto
        x_cam = np.clip(x_cam, -self.DUCT_HALF_SIZE, self.DUCT_HALF_SIZE)
        y_cam = np.clip(y_cam, -self.DUCT_HALF_SIZE, self.DUCT_HALF_SIZE)
        
        # Converte para coordenadas mundiais
        cam_point = np.array([x_cam, y_cam, z_cam, 1])
        world_point = camera_pose @ cam_point
        
        return world_point[:3]
    
    def estimate_camera_motion(self, frame, prev_frame):
        """
        Estima movimento da c√¢mera entre frames
        Movimento restrito pelas dimens√µes do duto
        """
        if prev_frame is None:
            return np.eye(4)
        
        # Movimento t√≠pico do drone no duto: principalmente para frente
        # Pequenos movimentos laterais e rota√ß√£o permitidos
        
        # Movimento padr√£o para frente (ajustar baseado na velocidade do drone)
        forward_motion = 0.03  # 3cm por frame (ajustar conforme FPS do v√≠deo)
        
        transform = np.eye(4)
        transform[2, 3] = forward_motion  # Movimento no eixo Z (para frente)
        
        # Pequenas varia√ß√µes laterais devido ao voo do drone
        # (Pode ser refinado com an√°lise de fluxo √≥ptico)
        lateral_noise = np.random.normal(0, 0.005, 2)  # ¬±5mm de ru√≠do
        transform[0, 3] = lateral_noise[0]  # X
        transform[1, 3] = lateral_noise[1]  # Y
        
        return transform
    
    def validate_session_positions(self, tracked_detections, camera_pose):
        """
        Valida se as posi√ß√µes das sess√µes fazem sentido no contexto do duto
        """
        valid_detections = []
        camera_pos = camera_pose[:3, 3]
        
        for detection in tracked_detections:
            track_id = detection['track_id']
            
            # Se j√° tem posi√ß√£o 3D, valida
            if track_id in self.session_positions:
                world_pos = self.session_positions[track_id]
                relative_pos = world_pos - camera_pos
                
                # Dist√¢ncia lateral do centro do duto
                lateral_distance = np.sqrt(relative_pos[0]**2 + relative_pos[1]**2)
                
                # Deve estar dentro do duto (com toler√¢ncia)
                if lateral_distance <= (self.DUCT_HALF_SIZE + self.DUCT_TOLERANCE):
                    # Profundidade razo√°vel
                    depth = relative_pos[2]
                    if 0.1 <= depth <= 3.0:
                        valid_detections.append(detection)
                        continue
            
            # Se n√£o tem posi√ß√£o 3D ainda, aceita por enquanto
            valid_detections.append(detection)
        
        return valid_detections
    
    def process_frame(self, frame, frame_idx):
        """
        Processa um frame individual para detectar sess√µes
        """
        frame_height, frame_width = frame.shape[:2]
        
        if self.camera_matrix is None:
            self.set_camera_calibration(frame_width, frame_height)
            self.frame_width = frame_width
            self.frame_height = frame_height
        
        # Executa detec√ß√£o/rastreamento YOLO
        results = self.model.track(
            frame,
            conf=self.confidence_threshold,
            persist=True,
            tracker="bytetrack.yaml",
            iou=0.3,  # IoU threshold
            max_det=self.MAX_SESSIONS_VISIBLE,
            verbose=False
        )
        
        # Anota o frame
        annotated_frame = results[0].plot()
        tracked_detections = []
        
        # Extrai informa√ß√µes das detec√ß√µes
        if results[0].boxes is not None and results[0].boxes.id is not None:
            boxes = results[0].boxes.xyxy.cpu().numpy()
            track_ids = results[0].boxes.id.cpu().numpy().astype(int)
            confidences = results[0].boxes.conf.cpu().numpy()
            classes = results[0].boxes.cls.cpu().numpy()
            
            for box, track_id, conf, cls in zip(boxes, track_ids, confidences, classes):
                center = ((box[0] + box[2]) / 2, (box[1] + box[3]) / 2)
                bbox_area = (box[2] - box[0]) * (box[3] - box[1])
                
                # Atualiza hist√≥rico
                self.track_history[track_id].append({
                    'frame': frame_idx,
                    'center': center,
                    'bbox': box,
                    'bbox_area': bbox_area,
                    'confidence': conf,
                    'class': int(cls)
                })
                
                tracked_detections.append({
                    'track_id': track_id,
                    'bbox': box,
                    'center': center,
                    'bbox_area': bbox_area,
                    'confidence': conf,
                    'class': int(cls),
                    'class_name': self.model.names[int(cls)]
                })
        
        return annotated_frame, tracked_detections
    
    def estimate_3d_positions(self, tracked_detections, camera_pose, frame_width, frame_height):
        """
        Estima posi√ß√µes 3D das sess√µes detectadas
        """
        for detection in tracked_detections:
            track_id = detection['track_id']
            center_2d = detection['center']
            bbox_area = detection['bbox_area']
            
            # Estima profundidade
            estimated_depth = self.estimate_depth_from_bbox(
                bbox_area, frame_width, frame_height)
            
            # Converte para coordenadas 3D
            world_pos = self.pixel_to_world_constrained(
                center_2d, estimated_depth, camera_pose, frame_width, frame_height)
            
            # Aplica suaviza√ß√£o temporal
            if track_id in self.session_positions:
                prev_pos = self.session_positions[track_id]
                
                # Limita movimento entre frames
                movement = np.linalg.norm(world_pos - prev_pos)
                if movement > self.MAX_FRAME_MOVEMENT:
                    # Limita movimento m√°ximo
                    direction = (world_pos - prev_pos) / movement
                    world_pos = prev_pos + direction * self.MAX_FRAME_MOVEMENT
                
                # Suaviza√ß√£o (filtro passa-baixa)
                alpha = 0.1  # Fator de suaviza√ß√£o
                self.session_positions[track_id] = (
                    alpha * world_pos + (1 - alpha) * prev_pos
                )
            else:
                self.session_positions[track_id] = world_pos
            
            print(f"üìç Sess√£o {track_id}: profundidade={estimated_depth:.2f}m, "
                  f"posi√ß√£o=({world_pos[0]:.2f}, {world_pos[1]:.2f}, {world_pos[2]:.2f})")
    
    def draw_duct_info(self, frame, tracked_detections):
        """
        Desenha informa√ß√µes do duto e sess√µes no frame
        """
        h, w = frame.shape[:2]
        
        # Desenha limites do duto (aproximado)
        margin = 30  # pixels
        cv2.rectangle(frame, (margin, margin), (w-margin, h-margin), (100, 100, 100), 2)
        
        # Linha central do duto
        cv2.line(frame, (w//2, 0), (w//2, h), (100, 100, 100), 1)
        cv2.line(frame, (0, h//2), (w, h//2), (100, 100, 100), 1)
        
        # Informa√ß√µes das sess√µes
        for detection in tracked_detections:
            track_id = detection['track_id']
            center = detection['center']
            confidence = detection['confidence']
            
            # Label da sess√£o
            if track_id in self.session_positions:
                world_pos = self.session_positions[track_id]
                depth = world_pos[2]
                label = f"S{track_id}: {depth:.2f}m ({confidence:.2f})"
            else:
                label = f"S{track_id}: nova ({confidence:.2f})"
            
            # Desenha label
            cv2.putText(frame, label, 
                       (int(center[0]), int(center[1]) - 15),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 2)
        
        # Informa√ß√µes gerais
        info_text = [
            f"Duto: {self.DUCT_SIZE}x{self.DUCT_SIZE}m",
            f"Sess√µes detectadas: {len(self.session_positions)}",
            f"Confian√ßa m√≠nima: {self.confidence_threshold:.2f}",
            f"FPS: {self.avg_fps:.1f}"
        ]
        
        y_offset = 30
        for text in info_text:
            cv2.putText(frame, text, (10, y_offset), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
            y_offset += 25
    
    def update_fps(self):
        """Atualiza c√°lculo de FPS"""
        self.fps_counter += 1
        if self.fps_counter % 10 == 0:  # Atualiza a cada 10 frames
            current_time = time.time()
            elapsed = current_time - self.fps_start_time
            self.avg_fps = 10 / elapsed if elapsed > 0 else 0
            self.fps_start_time = current_time
    
    def save_reconstruction(self, output_path):
        """Salva dados de reconstru√ß√£o 3D"""
        reconstruction_data = {
            'duct_size': self.DUCT_SIZE,
            'session_positions': {
                str(k): v.tolist() for k, v in self.session_positions.items()
            },
            'camera_poses': [pose.tolist() for pose in self.camera_poses],
            'total_sessions': len(self.session_positions)
        }
        
        with open(output_path, 'w') as f:
            json.dump(reconstruction_data, f, indent=2)
        
        print(f"üíæ Dados de reconstru√ß√£o salvos em: {output_path}")
    
    def run_video_inference(self, video_path, output_path=None, reconstruction_path=None, display=True):
        """
        Executa infer√™ncia em um v√≠deo com detec√ß√£o de sess√µes de dutos
        """
        cap = cv2.VideoCapture(video_path)
        
        if not cap.isOpened():
            print(f"‚ùå Erro: N√£o foi poss√≠vel abrir o v√≠deo: {video_path}")
            return
        
        # Propriedades do v√≠deo
        fps = int(cap.get(cv2.CAP_PROP_FPS))
        frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        print(f"üìπ Processando v√≠deo:")
        print(f"   Arquivo: {video_path}")
        print(f"   Resolu√ß√£o: {frame_width}x{frame_height}")
        print(f"   FPS: {fps}")
        print(f"   Total de frames: {total_frames}")
        
        # Configura√ß√£o de sa√≠da
        out = None
        if output_path:
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))
        
        frame_count = 0
        prev_frame = None
        
        print("\nüöÄ Iniciando processamento...")
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            frame_count += 1
            
            # Estima movimento da c√¢mera
            if prev_frame is not None:
                relative_pose = self.estimate_camera_motion(frame, prev_frame)
                current_pose = self.camera_poses[-1] @ relative_pose
                self.camera_poses.append(current_pose)
            
            # Processa frame
            annotated_frame, tracked_detections = self.process_frame(frame, frame_count)
            
            # Valida detec√ß√µes
            valid_detections = self.validate_session_positions(
                tracked_detections, self.camera_poses[-1])
            
            # Estima posi√ß√µes 3D
            if valid_detections:
                self.estimate_3d_positions(
                    valid_detections, self.camera_poses[-1], frame_width, frame_height)
            
            # Desenha informa√ß√µes
            self.draw_duct_info(annotated_frame, valid_detections)
            
            # Salva frame
            if out:
                out.write(annotated_frame)
            
            # Exibe frame
            if display:
                cv2.imshow('Detec√ß√£o de Sess√µes de Dutos', annotated_frame)
                if cv2.waitKey(1) & 0xFF == ord('q'):
                    break
            
            # Atualiza FPS
            self.update_fps()
            
            # Progress
            if frame_count % 100 == 0:
                progress = (frame_count / total_frames) * 100
                print(f"üìä Progresso: {progress:.1f}% ({frame_count}/{total_frames})")
            
            prev_frame = frame.copy()
        
        # Limpeza
        cap.release()
        if out:
            out.release()
        cv2.destroyAllWindows()
        
        # Salva reconstru√ß√£o
        if reconstruction_path:
            self.save_reconstruction(reconstruction_path)
        
        # Relat√≥rio final
        print(f"\n‚úÖ Processamento conclu√≠do!")
        print(f"üìä Estat√≠sticas finais:")
        print(f"   Frames processados: {frame_count}")
        print(f"   Sess√µes detectadas: {len(self.session_positions)}")
        print(f"   FPS m√©dio: {self.avg_fps:.1f}")
        
        return self.session_positions
    
    def run_webcam_inference(self, camera_id=0):
        """
        Executa infer√™ncia em tempo real com webcam
        """
        cap = cv2.VideoCapture(camera_id)
        
        if not cap.isOpened():
            print(f"‚ùå Erro: N√£o foi poss√≠vel abrir a c√¢mera {camera_id}")
            return
        
        print(f"üìπ Infer√™ncia em tempo real - Pressione 'q' para sair")
        
        frame_count = 0
        prev_frame = None
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            frame_count += 1
            
            # Estima movimento
            if prev_frame is not None:
                relative_pose = self.estimate_camera_motion(frame, prev_frame)
                current_pose = self.camera_poses[-1] @ relative_pose
                self.camera_poses.append(current_pose)
            
            # Processa
            annotated_frame, tracked_detections = self.process_frame(frame, frame_count)
            valid_detections = self.validate_session_positions(
                tracked_detections, self.camera_poses[-1])
            
            if valid_detections:
                frame_height, frame_width = frame.shape[:2]
                self.estimate_3d_positions(
                    valid_detections, self.camera_poses[-1], frame_width, frame_height)
            
            self.draw_duct_info(annotated_frame, valid_detections)
            self.update_fps()
            
            cv2.imshow('Detec√ß√£o de Sess√µes - Tempo Real', annotated_frame)
            
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
            
            prev_frame = frame.copy()
        
        cap.release()
        cv2.destroyAllWindows()


def main():
    """Fun√ß√£o principal"""
    parser = argparse.ArgumentParser(
        description='Sistema de Identifica√ß√£o de Sess√µes de Dutos',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemplos de uso:
  # Processar v√≠deo b√°sico
  python duct_detector.py -m modelo.pt -v video.mp4
  
  # Processar com sa√≠da e reconstru√ß√£o 3D
  python duct_detector.py -m modelo.pt -v video.mp4 -o resultado.mp4 -r reconstrucao.json
  
  # C√¢mera em tempo real
  python duct_detector.py -m modelo.pt -c 0
  
  # Ajustar confian√ßa
  python duct_detector.py -m modelo.pt -v video.mp4 --confidence 0.7
        """)
    
    parser.add_argument('--model', '-m', required=True,
                       help='Caminho para o modelo YOLO treinado (.pt)')
    parser.add_argument('--video', '-v',
                       help='Caminho para o arquivo de v√≠deo')
    parser.add_argument('--camera', '-c', type=int,
                       help='ID da c√¢mera (default: 0)')
    parser.add_argument('--output', '-o',
                       help='Caminho para salvar v√≠deo de sa√≠da')
    parser.add_argument('--confidence', '--conf', type=float, default=0.5,
                       help='Limiar de confian√ßa (default: 0.5)')
    parser.add_argument('--no-display', action='store_true',
                       help='Desabilita exibi√ß√£o em tempo real')
    parser.add_argument('--reconstruction', '-r',
                       help='Caminho para salvar dados de reconstru√ß√£o 3D (JSON)')
    
    args = parser.parse_args()
    
    # Valida√ß√µes
    if not os.path.exists(args.model):
        print(f"‚ùå Erro: Arquivo do modelo n√£o encontrado: {args.model}")
        return
    
    if not args.video and args.camera is None:
        print("‚ùå Erro: Especifique --video ou --camera")
        parser.print_help()
        return
    
    if args.video and not os.path.exists(args.video):
        print(f"‚ùå Erro: Arquivo de v√≠deo n√£o encontrado: {args.video}")
        return
    
    # Inicializa detector
    print("üöÄ Inicializando Sistema de Detec√ß√£o de Sess√µes de Dutos")
    detector = DuctDetectorLocal(args.model, args.confidence)
    
    # Executa infer√™ncia
    try:
        if args.video:
            detector.run_video_inference(
                args.video, args.output, args.reconstruction, not args.no_display)
        else:
            detector.run_webcam_inference(args.camera)
    except KeyboardInterrupt:
        print("\nüõë Interrompido pelo usu√°rio")
    except Exception as e:
        print(f"‚ùå Erro durante execu√ß√£o: {e}")
        raise


if __name__ == "__main__":
    main()